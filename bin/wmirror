#!/usr/bin/env sh
#
# wmirror
#
# Mirror a website.

URL=$1
DIR=~/Downloads/_websites/

if [ ! -d "$DIR" ]
then
  echo mkdir -p "$DIR"
  mkdir -p "$DIR"
fi

_domain=$(printf "%s\n" "$URL" | awk -F/ '{print $3}')

if [ -z "$_domain" ]; then
  printf "Domain not found.\n"
  exit 1
fi

# allow S3 in order to include assets.
#
# TODO: make this possible to set with options.
_allowed_hosts="$_domain,s3.amazonaws.com"

cd "$DIR"

# Build Command:
_cmd="wget \
  -r \
  -H -D$_allowed_hosts \
  -l inf \
  -np \
  -k \
  -c \
  -N \
  -w1 \
  --random-wait \
  -e robots=off \
  --no-check-certificate \
  $URL"

# Debug Command:
# printf "%s\n" "$_cmd"

# Call Command:
$_cmd

# -r                      recursive
# -H                      follow links to other hosts
# -D                      limit to the specified host list
# -l inf                  all sub directories
# -np                     do NOT crawl parent directories
# -k                      convert links (to refer to local directories)
# -c                      continue (script is restartable)
# -N                      only retrieve files when newer than local file
# -w1                     wait interval is 1 second
# --random-wait           random wait between 0..2 seconds (2 * wait interval)
# --e robots=off          ignore robots.txt (naughty)
# --no-check-certificate  live dangerously
